# Self-Attention 구현하기
이전 주차에서 어텐션을 활용한 모델들을 배워봤습니다. 토치가 제공하는 트랜스포머를 활용해보고, 기존 RNN 모델에 어텐션도 추가해 보았습니다.

하지만, 실제로 Attention is All you need 논문에서 제시된 트랜스포머를 직접 구현해본적이 없네요.

현대 LLM, 특히 새로운 딥시크 모델들은 독특한 어텐션 메커니즘을 사용합니다. 이런 것들을 구현하기 위해서는, 기존 Self-Attention 메커니즘인 Scaled-dot product attention에 대한 깊은 이해가 필요합니다.

이번 주차에서는 넘파이를 활용해 입력 데이터에 대한 SDPA를 구현해보겠습니다.

해당 링크에서 푸세요: [Deep-ml 53](https://www.deep-ml.com/problems/53)

## 요구사항
어텐션 메커니즘은 다음과 같은 과정을 거쳐 연산을 진행합니다.

0. 각 토큰의 임베딩 입력
주어진 문제에서는 이를 처리할 필요가 없습니다. 입력되는 토큰이 이미 임베딩을 받은 상태입니다.

1. Q, K, V 생성
받은 입력 X를 바탕으로, W^Q, W^K, W^V에 각각 곱해주어 Q, K, V를 생성하는 함수가 필요합니다.

compute_qkv라는 함수를 만들어 주세요.

2. SDPA 계산
Q, K, V를 받아, SDPA를 계산하는 함수 self_attention을 완성해 주세요.

Q와 K의 전치된 것을 서로 내적하고, 각 임베딩의 크기에 루트를 씌운 값을 나누어 줍니다.
이를 V와 내적한걸 반환하면 됩니다.

## 평가 기준
드디어 명확한 답이 존재하는 문제입니다!

실행하시면 확인할 수 있습니다.
실수로 전치를 까먹지 않도록 주의해주세요.