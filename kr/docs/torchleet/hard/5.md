# Seq2Seq 모델에 어텐션 추가하기
주의! Attention is all you need 논문이 아닙니다. 트랜스포머가 아니라는 것입니다.
Q, K, V와 같은 방식이 아니니, 명심해주세용.

본래 Attention은 RNN 기반 모델들의 한계를 극복하기 위해 사용된 것입니다.
이 문제는 LSTM에 추가로 어텐션을 메커니즘을 활용하는 것을 연습해는 부분입니다.

모델의 구조는 인코더와 디코더로 구성되어있고, 인코더는 단순한 LSTM, 디코더는 어텐션을 추가한 LSTM으로 구성되어있습니다.

## 요구사항
### 인코더
- 입력 시퀀스 -> 임베딩 처리
- LSTM에 입력(batch 조심)
- cell state와 함께 반환
### 디코더
어텐션을 적용해야 합니다.
- 입력 시퀀스 -> 임베딩
    - 이때 새로 들어오는게 딱 한개라서, unsqueeze(1) 해줘야 합니다.
- Linear 기반 어텐션이 필요하다.
    - 이때, 어텐션의 차원에서 이전 인코더의 hidden state와, 새로 만든 입력의 임베딩 결과를 서로 concat 합니다. (hidden_dim + embed_dim)
    - 합쳐져서 차원이 커진걸 다시 압축하는것도 필요합니다(임베딩 차원 수로 돌리기)
    - 이거에 softmax를 적용하고, context vector를 1번째 차원으로 unsqueeze해 한개 차원으로 만들면 (batch, 1, embed_dim)이 됩니다. 이걸 또 다시 embedded와 context_vector를 1번 차원(시퀀스 길이)를 squeeze한다음에 합쳐줌
    - 마지막으로 합쳐진 어텐션을 처리하는 Linear를 거치고, 다시 또 1번 차원을 unsqueeze해 시퀀스를 만들어줌.
- LSTM에 입력
    - 한개짜리 시퀀스를 가진 걸 LSTM에 입력한다.
- 출력 시퀀스를 반환
    - 최종 출력용 Linear를 거쳐야 한다.
