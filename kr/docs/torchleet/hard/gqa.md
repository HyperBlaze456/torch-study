# MHA/GQA
Attention is all you need 논문에서 제시된 MHA는 여러개의 헤드로 임베딩을 쪼개서 연산해 각 헤드가 각각 다른 특징을 학습하도록 하는 방법입니다.
저희가 이전에 사용했던 토치의 기본 어텐션은 여러 헤드를 사용하지 않았습니다. 논문과 다소 다르게 구현되었다고 볼 수 있죠.

이번주차는 LLM에 익숙하다면 꼭 들어보셨을 이 MHA와, KV 캐시를 감축하기 위한 GQA 아키텍처를 구현해보도록 하겠습니다.

## 정보
### MHA
MHA는 여러개의 헤드로 임베딩을 쪼개서 연산해 각 헤드가 각각 다른 특징을 학습하도록 하는 방법입니다.

Q, K, V를 거친 값에서, 임베딩을 각각 헤드 개수만큼 쪼개서 연산하고, 다시 합치는 방식입니다. 모두 같은 크기로 쪼개기 때문에 연산이 간단한 편입니다.
물론, 단순한 행렬곱은 아니겠지만요.

임베딩 차원의 크기는 반드시 헤드 개수로 나누어 떨어져야 합니다. 지워지면 답이 없어요!

### GQA
GQA는 KV 캐시를 감축하기 위한 아키텍처입니다. 기존 MHA의 경우, 모든 KV 캐시를 살려야 합니다. 임베딩 차원의 크기와 시퀀스 길이에 모두 정비례하기 때문에, 모델이 커질수록 메모리 사용량이 기하급수적으로 늘어나게 됩니다.

따라서, 이를 해결하기 위해 K, V 헤드의 개수를 줄이고 각 KV 헤드당 여러개의 Q 헤드를 대응시켜 연산합니다. 이때, 모든 Q 헤드를 한개의 KV 헤드를 대응시키게 되면 MQA가 됩니다.

하지만, 이는 성능을 감소시키기에 KV 헤드를 한개로 두지는 않게 만듭니다.

여기서 문제가 있습니다. MHA, MQA, GQA 모두 헤드라는 새로운 차원이 생겨버립니다. 단순한 행렬곱을 활용하려면 iterative하게 접근해야 하는데, 이는 당연히 성능을 감소시킵니다.
이를 해결하기 위해, 단순한 내적 연산 이상의 무언가가 필요합니다.

이를 해결하기 위해 einops, einsum을 활용할 수 있습니다. 지능적인 자동 연산으로, 이를 구현하는데 필요한 고민을 모두 해결해줍니다.

## 요구사항
MHA, MQA, GQA 이 세개 종류의 어텐션 방법을 구현해 보세요.

각각 구현해도 좋고, 아니면 한번에 사용할 수 있는 것도 좋습니다.

이 문제는 어텐션을 실제로 사용해 모델을 학습하지는 않을 것입니다.

